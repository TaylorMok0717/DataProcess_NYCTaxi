{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed695774-c71a-4c9a-b13d-a3646e32692c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Part 1 \n",
    "# 2. read the files from the Azure storage and make a copy of it into DBFS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "519b02b7-0d1c-415c-b2e0-f3dbb0c2e32f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"at2\"\n",
    "storage_account_access_key = \"EnterYourAccessKeyHere\"\n",
    "blob_container_name = \"at2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b0075bf-b4e2-434b-abe8-27d551bb7881",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-3252437749638083>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m \u001b[43mdbutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmount\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwasbs://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mblob_container_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m@\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstorage_account_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.blob.core.windows.net\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmount_point\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mnt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mblob_container_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mextra_configs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfs.azure.account.key.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstorage_account_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.blob.core.windows.net\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_account_access_key\u001b[49m\u001b[43m}\u001b[49m\n",
       "\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001b[0m, in \u001b[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m    360\u001b[0m exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m    361\u001b[0m exc\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
       "\n",
       "\u001b[0;31mExecutionError\u001b[0m: An error occurred while calling o886.mount.\n",
       ": java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/at2; nested exception is: \n",
       "\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/at2\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1057)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1083)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:204)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1077)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/at2\n",
       "\tat scala.Predef$.require(Predef.scala:281)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:580)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:948)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:729)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:937)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:588)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:120)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.CEMountHandler.receive(MountHandler.scala:172)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$5(DbfsServerBackend.scala:388)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:388)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:332)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:668)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:686)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:663)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1015)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:936)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:540)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:54)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:412)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:169)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n",
       "\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:83)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:49)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:78)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n",
       "\t... 1 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)\nFile \u001b[0;32m<command-3252437749638083>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdbutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmount\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwasbs://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mblob_container_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m@\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstorage_account_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.blob.core.windows.net\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmount_point\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mnt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mblob_container_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mextra_configs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfs.azure.account.key.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstorage_account_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.blob.core.windows.net\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_account_access_key\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\nFile \u001b[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001b[0m, in \u001b[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    361\u001b[0m exc\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n\n\u001b[0;31mExecutionError\u001b[0m: An error occurred while calling o886.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/at2; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/at2\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1057)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1083)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:204)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1077)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/at2\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:580)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:948)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:729)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:937)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:588)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:120)\n\tat com.databricks.backend.daemon.data.server.handler.CEMountHandler.receive(MountHandler.scala:172)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$5(DbfsServerBackend.scala:388)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:388)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:332)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:668)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:686)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:663)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1015)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:936)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:540)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:412)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:54)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:412)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:169)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:83)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:49)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:78)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n",
       "errorSummary": "java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/at2; nested exception is: ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.mount(\n",
    "  source = f'wasbs://{blob_container_name}@{storage_account_name}.blob.core.windows.net',\n",
    "  mount_point = f'/mnt/{blob_container_name}/',\n",
    "  extra_configs = {'fs.azure.account.key.' + storage_account_name + '.blob.core.windows.net': storage_account_access_key}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad867690-95dc-4ca5-a6f0-fed6b92c01c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/mnt/at2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8267c870-04e1-4d75-8ce4-1a8599ee3428",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"/mnt/at2\", \"/dbfs/FileStore\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c249c41-b3c5-4a44-9a46-e580b070c415",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Count the total numbers of rows for each taxi colour (yellow and green) by reading the files stored on DBFS\n",
    "# Read Parquet files for yellow taxis\n",
    "yellow_df = spark.read.parquet(\"dbfs:/dbfs/FileStore/\" +  \"yellow_taxi_*.parquet\")\n",
    "\n",
    "# Read Parquet files for green taxis\n",
    "green_df = spark.read.parquet(\"dbfs:/dbfs/FileStore/\" +  \"green_taxi_*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64de23eb-0b6e-41b9-b58d-9c38ea7aad94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow taxi count: 663055251\n",
      "Green taxi count: 66200401\n"
     ]
    }
   ],
   "source": [
    "# count number of row\n",
    "from pyspark.sql.functions import count\n",
    "yellow_count = yellow_df.count()\n",
    "green_count = green_df.count()\n",
    "print(\"Yellow taxi count:\", yellow_count)\n",
    "print(\"Green taxi count:\", green_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7073cd8-93fa-4a7e-9696-b5b27053d361",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the location referential csv\n",
    "taxi_zone_lookup = spark.read.option(\"header\", True).csv(\"dbfs:/FileStore/taxi_zone_lookup-1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c6565bb-1930-4ad1-83ca-277b6295d7f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_zone_lookup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4a45fd-062b-4eee-810c-ab328d9f8d67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. Convert the “Green” 2015 parquet into a csv file\n",
    "green_taxi_2015 = spark.read.option(\"header\", True).parquet(\"/dbfs/FileStore/green_taxi_2015.parquet\")\n",
    "green_taxi_2015.write.option(\"header\", True).csv( \"/mnt/at2/green_taxi_2015.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2a2f64-573e-4b39-91d5-97b46824ec92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Data cleaning\n",
    "yellow_df_clean = yellow_df\n",
    "green_df_clean = green_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "befa0005-3361-47db-9e29-4c562599c11a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: bigint, lpep_pickup_datetime: timestamp, lpep_dropoff_datetime: timestamp, store_and_fwd_flag: string, RatecodeID: double, PULocationID: bigint, DOLocationID: bigint, passenger_count: double, trip_distance: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, ehail_fee: double, improvement_surcharge: double, total_amount: double, payment_type: double, trip_type: double, congestion_surcharge: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df_clean.write.parquet(\"dbfs:/FileStore/yellow_df.parquet\", mode=\"overwrite\")\n",
    "green_df_clean.write.parquet(\"dbfs:/FileStore/green_df.parquet\", mode=\"overwrite\")\n",
    "yellow_df_clean = spark.read.parquet(\"dbfs:/FileStore/yellow_df.parquet\")\n",
    "green_df_clean = spark.read.parquet(\"dbfs:/FileStore/green_df.parquet\")\n",
    "yellow_df_clean.cache()\n",
    "green_df_clean.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58268b7f-9d0d-4c03-a88f-2f28bd054358",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# a. Remove trips finishing before the starting time\n",
    "yellow_df_clean = yellow_df_clean[yellow_df_clean['tpep_dropoff_datetime'] > yellow_df_clean['tpep_pickup_datetime']]\n",
    "green_df_clean = green_df_clean[green_df_clean['lpep_dropoff_datetime'] > green_df_clean['lpep_pickup_datetime']]\n",
    "\n",
    "# Remove data with passenger count < 1\n",
    "yellow_df_clean = yellow_df_clean.filter(yellow_df_clean['passenger_count'] >= 1)\n",
    "green_df_clean = green_df_clean.filter(green_df_clean['passenger_count'] >= 1)\n",
    "\n",
    "# Remove trips with negtive charge amount or 0 fare or total amount\n",
    "yellow_df_clean = yellow_df_clean.filter(\n",
    "    (yellow_df_clean['fare_amount'] > 0) &\n",
    "    (yellow_df_clean['tolls_amount'] >= 0) &\n",
    "    (yellow_df_clean['extra'] >= 0) &\n",
    "    (yellow_df_clean['mta_tax'] >= 0) &\n",
    "    (yellow_df_clean['tip_amount'] >= 0) &\n",
    "    (yellow_df_clean['tolls_amount'] >= 0) &\n",
    "    (yellow_df_clean['improvement_surcharge'] >= 0) &\n",
    "    (yellow_df_clean['total_amount'] > 0 )&\n",
    "    (yellow_df_clean['airport_fee'] >= 0 ) &\n",
    "    (yellow_df_clean['congestion_surcharge'] >= 0 ) \n",
    "    )\n",
    "\n",
    "green_df_clean = green_df_clean.filter(\n",
    "    (green_df_clean['fare_amount'] > 0) &\n",
    "    (green_df_clean['tolls_amount'] >= 0) &\n",
    "    (green_df_clean['extra'] >= 0) &\n",
    "    (green_df_clean['mta_tax'] >= 0) &\n",
    "    (green_df_clean['tip_amount'] >= 0) &\n",
    "    (green_df_clean['tolls_amount'] >= 0) &\n",
    "    (green_df_clean['improvement_surcharge'] >= 0) &\n",
    "    (green_df_clean['total_amount'] > 0 ) &\n",
    "    (green_df_clean['congestion_surcharge'] >= 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "604e1322-989b-4580-a1bc-3891745a57a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Create a column with the trip duration in min\n",
    "yellow_df_clean = yellow_df_clean.withColumn(\n",
    "    \"trip_duration\",\n",
    "    expr(\"round((unix_timestamp(tpep_dropoff_datetime) - unix_timestamp(tpep_pickup_datetime)) / 60, 2)\")\n",
    ")\n",
    "\n",
    "green_df_clean = green_df_clean.withColumn(\n",
    "    \"trip_duration\",\n",
    "    expr(\"round((unix_timestamp(lpep_dropoff_datetime) - unix_timestamp(lpep_pickup_datetime)) / 60, 2)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "495543df-253b-40f9-ae60-d66ee4febcd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a column speed with unit km/hour \n",
    "yellow_df_clean = yellow_df_clean.withColumn(\n",
    "    \"speed\",\n",
    "    expr(\"round((trip_distance * 1.60934) / (trip_duration / 60), 3)\")\n",
    ")\n",
    "\n",
    "green_df_clean = green_df_clean.withColumn(\n",
    "    \"speed\",\n",
    "    expr(\"round((trip_distance * 1.60934) / (trip_duration / 60), 3)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9dd39b3-1ce4-400f-ba7b-a2907d926ea2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "yellow_df_clean = yellow_df_clean.filter(\n",
    "    (yellow_df_clean['trip_duration'] >= 1) &\n",
    "    (yellow_df_clean['trip_duration'] <= 240) &\n",
    "    (yellow_df_clean['trip_distance'] >= 0.2 ) &\n",
    "    (yellow_df_clean['trip_distance'] <= 60 ) &\n",
    "    (yellow_df_clean['speed'] > 0) &\n",
    "    (yellow_df_clean['speed'] <= 55 )\n",
    ")\n",
    "\n",
    "green_df_clean = green_df_clean.filter(\n",
    "    (green_df_clean['trip_duration'] >= 1) &\n",
    "    (green_df_clean['trip_duration'] <= 240) &\n",
    "    (green_df_clean['trip_distance'] >= 0.2 ) &\n",
    "    (green_df_clean['trip_distance'] <= 60 ) &\n",
    "    (green_df_clean['speed'] > 0) &\n",
    "    (green_df_clean['speed'] <= 55 )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed8fabc-5a45-4938-8d29-087f3cd7ebf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rename \n",
    "green_df_clean = green_df_clean.withColumnRenamed(\"lpep_pickup_datetime\", \"tpep_pickup_datetime\")\n",
    "green_df_clean = green_df_clean.withColumnRenamed(\"lpep_dropoff_datetime\", \"tpep_dropoff_datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b49cfe7-4f9d-4b01-92c7-54afa266c47d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "green_df_clean = green_df_clean.withColumn(\"taxi_type\", lit(\"green\"))\n",
    "yellow_df_clean = yellow_df_clean.withColumn(\"taxi_type\", lit(\"yellow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "882a7a8a-afd7-427f-8279-ed62246801db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType,IntegerType,StringType\n",
    "\n",
    "yellow_df_clean = yellow_df_clean.withColumn(\"ehail_fee\", lit(None).cast(DoubleType()))\n",
    "yellow_df_clean = yellow_df_clean.withColumn(\"trip_type\", lit(None).cast(DoubleType()))\n",
    "\n",
    "# Add missing columns to cleaned_green_taxi_df and fill with nulls\n",
    "green_df_clean = green_df_clean.withColumn(\"airport_fee\", lit(None).cast(DoubleType())) \n",
    "green_df_clean = green_df_clean.withColumn(\"payment_type \", lit(None).cast(IntegerType()))\n",
    "green_df_clean = green_df_clean.withColumn(\"store_and_fwd_flag\", lit(None).cast(StringType()))\n",
    "\n",
    "combined_df = yellow_df_clean.union(green_df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78ae0ce1-3fd1-4bd4-bfbc-26cbb6ad201e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_df = combined_df.join(\n",
    "    taxi_zone_lookup,\n",
    "    on=(combined_df['PULocationID'] == taxi_zone_lookup['LocationID']),\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fd2bfa8-eec4-4752-8b38-e2e743af5ba8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_df = combined_df.withColumnRenamed(\"Borough\", \"pickup_Borough\") \\\n",
    "                     .withColumnRenamed(\"Zone\", \"pickup_Zone\") \\\n",
    "                     .withColumnRenamed(\"service_zone\", \"pickup_service_zone\")\n",
    "                     \n",
    "combined_df = combined_df.drop('LocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1ddebc-e844-4b0a-aedf-b10569179731",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_df = combined_df.join(\n",
    "    taxi_zone_lookup,\n",
    "    on=(combined_df['DOLocationID'] == taxi_zone_lookup['LocationID']),\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb420f3-11ce-4d82-a1ea-16d4af8306c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_df = combined_df.withColumnRenamed(\"Borough\", \"dropoff_Borough\") \\\n",
    "                     .withColumnRenamed(\"Zone\", \"dropoff_Zone\") \\\n",
    "                     .withColumnRenamed(\"service_zone\", \"dropoff_service_zone\")\n",
    "                     \n",
    "combined_df = combined_df.drop('LocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f12937f-8a54-486a-b066-40f32cf7fe7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>airport_fee</th><th>trip_duration</th><th>speed</th><th>taxi_type</th><th>ehail_fee</th><th>trip_type</th><th>pickup_Borough</th><th>pickup_Zone</th><th>pickup_service_zone</th><th>dropoff_Borough</th><th>dropoff_Zone</th><th>dropoff_service_zone</th></tr></thead><tbody><tr><td>1</td><td>2018-01-11T22:26:54.000+0000</td><td>2018-01-11T22:47:58.000+0000</td><td>1.0</td><td>2.1</td><td>1.0</td><td>Y</td><td>141.0</td><td>238.0</td><td>1.0</td><td>14.5</td><td>2.5</td><td>0.5</td><td>2.56</td><td>0.0</td><td>0.3</td><td>20.36</td><td>2.0</td><td>0.0</td><td>21.07</td><td>9.624</td><td>yellow</td><td>null</td><td>null</td><td>Manhattan</td><td>Lenox Hill West</td><td>Yellow Zone</td><td>Manhattan</td><td>Upper West Side North</td><td>Yellow Zone</td></tr><tr><td>1</td><td>2018-01-11T23:56:19.000+0000</td><td>2018-01-12T00:03:56.000+0000</td><td>1.0</td><td>1.0</td><td>1.0</td><td>Y</td><td>186.0</td><td>234.0</td><td>1.0</td><td>7.0</td><td>2.5</td><td>0.5</td><td>2.05</td><td>0.0</td><td>0.3</td><td>12.35</td><td>2.0</td><td>0.0</td><td>7.62</td><td>12.672</td><td>yellow</td><td>null</td><td>null</td><td>Manhattan</td><td>Penn Station/Madison Sq West</td><td>Yellow Zone</td><td>Manhattan</td><td>Union Sq</td><td>Yellow Zone</td></tr><tr><td>1</td><td>2018-01-12T00:47:17.000+0000</td><td>2018-01-12T00:55:40.000+0000</td><td>1.0</td><td>1.8</td><td>1.0</td><td>Y</td><td>239.0</td><td>43.0</td><td>1.0</td><td>8.5</td><td>2.5</td><td>0.5</td><td>2.35</td><td>0.0</td><td>0.3</td><td>14.15</td><td>2.0</td><td>0.0</td><td>8.38</td><td>20.741</td><td>yellow</td><td>null</td><td>null</td><td>Manhattan</td><td>Upper West Side South</td><td>Yellow Zone</td><td>Manhattan</td><td>Central Park</td><td>Yellow Zone</td></tr><tr><td>1</td><td>2018-01-12T01:06:08.000+0000</td><td>2018-01-12T01:08:21.000+0000</td><td>1.0</td><td>0.4</td><td>1.0</td><td>Y</td><td>237.0</td><td>237.0</td><td>1.0</td><td>3.5</td><td>2.5</td><td>0.5</td><td>1.7</td><td>0.0</td><td>0.3</td><td>8.5</td><td>2.0</td><td>0.0</td><td>2.22</td><td>17.398</td><td>yellow</td><td>null</td><td>null</td><td>Manhattan</td><td>Upper East Side South</td><td>Yellow Zone</td><td>Manhattan</td><td>Upper East Side South</td><td>Yellow Zone</td></tr><tr><td>1</td><td>2018-01-12T01:36:42.000+0000</td><td>2018-01-12T01:59:20.000+0000</td><td>1.0</td><td>4.4</td><td>1.0</td><td>Y</td><td>249.0</td><td>263.0</td><td>1.0</td><td>17.5</td><td>2.5</td><td>0.5</td><td>4.15</td><td>0.0</td><td>0.3</td><td>24.95</td><td>2.0</td><td>0.0</td><td>22.63</td><td>18.774</td><td>yellow</td><td>null</td><td>null</td><td>Manhattan</td><td>West Village</td><td>Yellow Zone</td><td>Manhattan</td><td>Yorkville West</td><td>Yellow Zone</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "2018-01-11T22:26:54.000+0000",
         "2018-01-11T22:47:58.000+0000",
         "1.0",
         2.1,
         1,
         "Y",
         141,
         238,
         1,
         14.5,
         2.5,
         0.5,
         2.56,
         0,
         0.3,
         20.36,
         2,
         0,
         21.07,
         9.624,
         "yellow",
         null,
         null,
         "Manhattan",
         "Lenox Hill West",
         "Yellow Zone",
         "Manhattan",
         "Upper West Side North",
         "Yellow Zone"
        ],
        [
         1,
         "2018-01-11T23:56:19.000+0000",
         "2018-01-12T00:03:56.000+0000",
         "1.0",
         1,
         1,
         "Y",
         186,
         234,
         1,
         7,
         2.5,
         0.5,
         2.05,
         0,
         0.3,
         12.35,
         2,
         0,
         7.62,
         12.672,
         "yellow",
         null,
         null,
         "Manhattan",
         "Penn Station/Madison Sq West",
         "Yellow Zone",
         "Manhattan",
         "Union Sq",
         "Yellow Zone"
        ],
        [
         1,
         "2018-01-12T00:47:17.000+0000",
         "2018-01-12T00:55:40.000+0000",
         "1.0",
         1.8,
         1,
         "Y",
         239,
         43,
         1,
         8.5,
         2.5,
         0.5,
         2.35,
         0,
         0.3,
         14.15,
         2,
         0,
         8.38,
         20.741,
         "yellow",
         null,
         null,
         "Manhattan",
         "Upper West Side South",
         "Yellow Zone",
         "Manhattan",
         "Central Park",
         "Yellow Zone"
        ],
        [
         1,
         "2018-01-12T01:06:08.000+0000",
         "2018-01-12T01:08:21.000+0000",
         "1.0",
         0.4,
         1,
         "Y",
         237,
         237,
         1,
         3.5,
         2.5,
         0.5,
         1.7,
         0,
         0.3,
         8.5,
         2,
         0,
         2.22,
         17.398,
         "yellow",
         null,
         null,
         "Manhattan",
         "Upper East Side South",
         "Yellow Zone",
         "Manhattan",
         "Upper East Side South",
         "Yellow Zone"
        ],
        [
         1,
         "2018-01-12T01:36:42.000+0000",
         "2018-01-12T01:59:20.000+0000",
         "1.0",
         4.4,
         1,
         "Y",
         249,
         263,
         1,
         17.5,
         2.5,
         0.5,
         4.15,
         0,
         0.3,
         24.95,
         2,
         0,
         22.63,
         18.774,
         "yellow",
         null,
         null,
         "Manhattan",
         "West Village",
         "Yellow Zone",
         "Manhattan",
         "Yorkville West",
         "Yellow Zone"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "VendorID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "tpep_pickup_datetime",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "tpep_dropoff_datetime",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "passenger_count",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "trip_distance",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "RatecodeID",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "store_and_fwd_flag",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PULocationID",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DOLocationID",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "payment_type",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "fare_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "extra",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "mta_tax",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tip_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tolls_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "improvement_surcharge",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "congestion_surcharge",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "airport_fee",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "trip_duration",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "speed",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "taxi_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ehail_fee",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "trip_type",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pickup_Borough",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "pickup_Zone",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "pickup_service_zone",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dropoff_Borough",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dropoff_Zone",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dropoff_service_zone",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(combined_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1eb2ace-d138-489c-9a53-fe718bcb1694",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_df.write.parquet(\"dbfs:/FileStore/combined_df.parquet\", mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "at2_part1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
